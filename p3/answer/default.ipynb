{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bertchunker: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [02:32<00:00,  6.72it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = FinetuneTagger(os.path.join('..', 'data', 'chunker'), modelsuffix='.pt')\n",
    "decoder_output = chunker.decode(os.path.join('..', 'data', 'input', 'dev.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the warnings from the transformers library. They are expected to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 12296 phrases; correct: 10648.\n",
      "accuracy:  92.51%; (non-O)\n",
      "accuracy:  92.87%; precision:  86.60%; recall:  89.51%; FB1:  88.03\n",
      "             ADJP: precision:  64.37%; recall:  74.34%; FB1:  68.99  261\n",
      "             ADVP: precision:  63.88%; recall:  76.88%; FB1:  69.78  479\n",
      "            CONJP: precision:  41.67%; recall:  71.43%; FB1:  52.63  12\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  18\n",
      "               NP: precision:  86.08%; recall:  89.82%; FB1:  87.91  6508\n",
      "               PP: precision:  97.01%; recall:  93.12%; FB1:  95.03  2343\n",
      "              PRT: precision:  64.41%; recall:  84.44%; FB1:  73.08  59\n",
      "             SBAR: precision:  75.71%; recall:  78.90%; FB1:  77.27  247\n",
      "              UCP: precision:   0.00%; recall:   0.00%; FB1:   0.00  10\n",
      "               VP: precision:  87.71%; recall:  89.80%; FB1:  88.74  2359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(86.59726740403383, 89.50907868190988, 88.02910052910053)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "sys.path.append('..')\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('..', 'data', 'reference', 'dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chanege 1\n",
    "we can adopt a more sophisticated approach than simply taking the tag of the first subword for the entire word. A better strategy could involve voting or averaging predictions across subwords associated with a single word. Since the voting mechanism is simpler and more straightforward for classification tasks like tagging, we'll implement that approach.\n",
    "\n",
    "    Subword Processing: This method processes each subword associated with a word, collects the predicted tags for all subwords of a word, and then decides on the final tag for the word based on a voting mechanism among its subwords.\n",
    "\n",
    "    Voting Mechanism: For each word, the predicted tags from its subwords are aggregated, and the most frequently predicted tag is chosen as the final tag for the word. This approach considers the contribution of each subword to the overall prediction, potentially leading to more accurate and consistent tagging.\n",
    "    \n",
    "    Error Handling: An assertion is added to ensure the length of the input sequence matches the length of the output predictions, enhancing the method's robustness.\n",
    "\n",
    "This improved method leverages the information from all subwords associated with a word, potentially increasing the accuracy of the tagging system by considering more context than simply the first subword's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(self, model, seq):\n",
    "    output = [[] for _ in seq]\n",
    "    with torch.no_grad():\n",
    "        inputs = self.prepare_sequence(seq).to(device)\n",
    "        tag_scores = model(inputs.input_ids).squeeze(0)\n",
    "        word_ids = inputs.encodings[0].word_ids  # Get word IDs for subwords\n",
    "        predictions = [[] for _ in range(len(seq))]  # Prepare a list to hold predictions for each word\n",
    "\n",
    "        # Iterate through each subword prediction\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:  # Ignore special tokens\n",
    "                # Append the predicted tag (as index) for each subword to the corresponding word's predictions\n",
    "                predictions[word_id].append(int(tag_scores[i].argmax(dim=0)))\n",
    "\n",
    "        # For each word, determine the most common predicted tag among its subwords\n",
    "        for i, word_predictions in enumerate(predictions):\n",
    "            if word_predictions:  # Ensure there are predictions to process\n",
    "                # Use a voting mechanism to decide on the tag for each word\n",
    "                most_common_tag_idx = max(set(word_predictions), key=word_predictions.count)\n",
    "                output[i] = self.ix_to_tag[most_common_tag_idx]\n",
    "\n",
    "    assert len(seq) == len(output), \"The length of the sequence and output do not match.\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he dev score is 90.3237"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specifically improve the subword-to-word resolution strategy in the argmax method as per the comments, let's explore a nuanced approach that aims to better capture the essence of the entire word from its constituent subwords. This involves a combination of strategies that could potentially address the drop in development scores:\n",
    "\n",
    "    Enhanced Majority Voting: Instead of a simple majority vote, consider the frequency and confidence (softmax probabilities) of predicted tags for a more informed decision.\n",
    "\n",
    "    Confidence-weighted Voting: Use the softmax probabilities to weight the votes for each tag, giving more influence to predictions made with higher confidence.\n",
    "\n",
    "    Hybrid Approach: Combine the first subword's tag with a voting mechanism for the rest of the subwords. This approach acknowledges the importance of the first subword in many BERT-like models while still considering the context provided by subsequent subwords.\n",
    "\n",
    "Implementing a confidence-weighted voting mechanism requires modifying the model's forward method to return softmax probabilities in addition to the argmax indices. However, to keep the modifications feasible within the original code structure and without altering the model's forward method, we'll focus on a hybrid approach that combines the insights from both the first subword and a simple majority vote for all subwords associated with a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(self, model, seq):\n",
    "    output = [''] * len(seq)  # Initialize output with empty strings for each word in the input sequence\n",
    "    with torch.no_grad():\n",
    "        inputs = self.prepare_sequence(seq).to(device)\n",
    "        tag_scores = model(inputs.input_ids).squeeze(0)\n",
    "        word_ids = inputs.encodings[0].word_ids  # Get word IDs for subwords\n",
    "        \n",
    "        # Initialize a list to hold all tag predictions for each word\n",
    "        word_tag_predictions = [[] for _ in range(len(seq))]\n",
    "        \n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:  # Ignore special tokens\n",
    "                predicted_tag_idx = int(tag_scores[i].argmax(dim=0))\n",
    "                word_tag_predictions[word_id].append(predicted_tag_idx)\n",
    "        \n",
    "        # Decide the tag for each word\n",
    "        for word_id, tag_idxs in enumerate(word_tag_predictions):\n",
    "            if tag_idxs:\n",
    "                # If the first subword's tag differs from the majority, consider both\n",
    "                first_tag = tag_idxs[0]\n",
    "                majority_tag = max(set(tag_idxs), key=tag_idxs.count)\n",
    "                \n",
    "                # Prioritize the first subword's tag if there's a tie or the list is dominated by a single tag\n",
    "                if first_tag == majority_tag or len(set(tag_idxs)) == 1:\n",
    "                    output[word_id] = self.ix_to_tag[first_tag]\n",
    "                else:\n",
    "                    # Hybrid approach: Consider the first subword's tag but note the presence of other tags\n",
    "                    output[word_id] = self.ix_to_tag[majority_tag]  # Fallback to majority if there's a clear preference\n",
    "                \n",
    "    assert len(seq) == len(output), \"The length of the sequence and output do not match.\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dev score 90.3237"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Label Weights Calculation: Before training, we calculate the weights for each label based on their frequency in the training data, which are then used to create a weighted NLLLoss. This helps the model pay more attention to infrequent labels.\n",
    "\n",
    "Freezing Encoder Layers: In the initial epochs, the encoder's parameters are set to not require gradients, effectively freezing those layers. This allows the model's classification head to adapt to the task without disruptive updates from the pre-trained layers. After a specified number of epochs, the encoder is unfrozen to fine-tune the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "        self.load_training_data(self.trainfile)\n",
    "        self.model = TransformerModel(self.basemodel, len(self.tag_to_ix), lr=self.lr).to(device)\n",
    "        # TODO You may want to set the weights in the following line to increase the effect of\n",
    "        #   gradients for infrequent labels and reduce the dominance of the frequent labels\n",
    "        # Calculate label weights for weighted loss\n",
    "        label_counts = np.zeros(len(self.tag_to_ix))\n",
    "        for _, tags in self.training_data:\n",
    "            for tag in tags:\n",
    "                label_counts[self.tag_to_ix[tag]] += 1\n",
    "        label_weights = 1.0 / (label_counts + 1e-6)  # Prevent division by zero\n",
    "        label_weights = label_weights / label_weights.sum() * len(self.tag_to_ix)  # Normalize\n",
    "        label_weights = torch.tensor(label_weights, dtype=torch.float).to(device)\n",
    "        \n",
    "        loss_function = nn.NLLLoss(weight=label_weights)\n",
    "        self.model.train()\n",
    "        #loss = float(\"inf\")\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            if epoch < 2:  \n",
    "                for param in self.model.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:  \n",
    "                for param in self.model.encoder.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            train_iterator = tqdm.tqdm(self.training_data)\n",
    "            batch = []\n",
    "            for tokenized_sentence, tags in train_iterator:\n",
    "                # Step 1. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of subword indices. Pre-trained transformer based models come with their fixed\n",
    "                # input tokenizer which in our case will receive the words in a sentence and will convert the words list\n",
    "                # into a list of subwords (e.g. you can look at https://aclanthology.org/P16-1162.pdf to get a better\n",
    "                # understanding about BPE subword vocabulary creation technique).\n",
    "                # The expected labels will be copied as many times as the size of the subwords list for each word and\n",
    "                # returned in targets label.\n",
    "                batch.append(self.prepare_sequence(tokenized_sentence, tags))\n",
    "                if len(batch) < self.batchsize:\n",
    "                    continue\n",
    "                pad_id = self.tokenizer.pad_token_id\n",
    "                o_id = self.tag_to_ix['O']\n",
    "                max_len = max([x[1].size(0) for x in batch])\n",
    "                # in the next two lines we pad the batch items so that each sequence comes to the same size before\n",
    "                #  feeding the input batch to the model and calculating the loss over the target values.\n",
    "                input_batch = [x[0].input_ids[0].tolist() + [pad_id] * (max_len - x[0].input_ids[0].size(0)) for x in batch]\n",
    "                target_batch = [x[1].tolist() + [o_id] * (max_len - x[0].input_ids[0].size(0)) for x in batch]\n",
    "                sentence_in = torch.LongTensor(input_batch).to(device)\n",
    "                targets = torch.LongTensor(target_batch).to(device)\n",
    "                # Step 2. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.model.zero_grad()\n",
    "                # Step 3. Run our forward pass.\n",
    "                tag_scores = self.model(sentence_in)\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores.view(-1, len(self.tag_to_ix)), targets.view(-1))\n",
    "                total_loss += loss.item()\n",
    "                loss_count += 1\n",
    "                loss.backward()\n",
    "                # TODO you may want to freeze the BERT encoder for a couple of epochs\n",
    "                #   and then start performing full fine-tuning.\n",
    "                for optimizer in self.model.optimizers:\n",
    "                    optimizer.step()\n",
    "                # HINT: getting the value of loss below 2.0 might mean your model is moving in the right direction!\n",
    "                train_iterator.set_description(f\"loss: {total_loss/loss_count:.3f}\")\n",
    "                del batch[:]\n",
    "\n",
    "            if epoch == self.epochs - 1:\n",
    "                epoch_str = '' \n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            print(f\"Saving model file: {savefile}\", file=sys.stderr)\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.model.optimizers[0].state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'tag_to_ix': self.tag_to_ix,\n",
    "                        'ix_to_tag': self.ix_to_tag,\n",
    "                    }, savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dev score is 90.5883"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the TransformerModel class involves several enhancements, particularly focusing on initializing a Conditional Random Field (CRF) layer for sequence labeling tasks and adjusting the optimization strategy to accommodate different learning rates for different parts of the model. \n",
    "Adding a CRF Layer\n",
    "\n",
    "A CRF layer on top of the classification output can significantly improve the model's performance on sequence labeling tasks by considering the dependencies between labels in a sequence. The torchcrf library provides a PyTorch implementation of CRF which can be used here.\n",
    "\n",
    "    Initialize the CRF Layer: After the classification head, initialize a CRF layer that will operate on the logits provided by the classification head.\n",
    "\n",
    "Adjusting Optimizers\n",
    "\n",
    "Different parts of the model might benefit from being optimized at different learning rates. For example, the pre-trained encoder might require a smaller learning rate to avoid catastrophic forgetting, while the newly added layers (like the classification head and CRF layer) might use a higher learning rate to speed up their convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            basemodel,\n",
    "            tagset_size,\n",
    "            lr=5e-5\n",
    "        ):\n",
    "        torch.manual_seed(1)\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.basemodel = basemodel\n",
    "        # the encoder will be a BERT-like model that receives an input text in subwords and maps each subword into\n",
    "        # contextual representations\n",
    "        self.encoder = None\n",
    "        # the hidden dimension of the BERT-like model will be automatically set in the init function!\n",
    "        self.encoder_hidden_dim = 0\n",
    "        # The linear layer that maps the subword contextual representation space to tag space\n",
    "        self.classification_head = None\n",
    "        # The CRF layer on top of the classification head to make sure the model learns to move from/to relevant tags\n",
    "        # self.crf_layer = None\n",
    "        # optimizers will be initialized in the init_model_from_scratch function\n",
    "        self.optimizers = None\n",
    "        self.init_model_from_scratch(basemodel, tagset_size, lr)\n",
    "\n",
    "    def init_model_from_scratch(self, basemodel, tagset_size, lr):\n",
    "        self.encoder = AutoModel.from_pretrained(basemodel)\n",
    "        self.encoder_hidden_dim = self.encoder.config.hidden_size\n",
    "        self.classification_head = nn.Linear(self.encoder_hidden_dim, tagset_size)\n",
    "        # TODO initialize self.crf_layer in here as well.\n",
    "        self.crf_layer = CRF(tagset_size, batch_first=True)\n",
    "        # TODO modify the optimizers in a way that each model part is optimized with a proper learning rate!\n",
    "        encoder_params = list(self.encoder.parameters())\n",
    "        classifier_params = list(self.classification_head.parameters()) + list(self.crf_layer.parameters())\n",
    "        self.optimizers = {\n",
    "            'encoder': optim.Adam(encoder_params, lr=lr * 0.1),  # Lower lr for the encoder\n",
    "            'classifier': optim.Adam(classifier_params, lr=lr)   # Specified lr for classifier and CRF\n",
    "        }\n",
    "\n",
    "    def forward(self, sentence_input, labels=None):\n",
    "        encoded = self.encoder(sentence_input).last_hidden_state\n",
    "        tag_space = self.classification_head(encoded)\n",
    "        #tag_scores = F.log_softmax(tag_space, dim=-1)\n",
    "        # TODO modify the tag_scores to use the parameters of the crf_layer\n",
    "        if labels is not None:\n",
    "            loss = -self.crf_layer(tag_space, labels, reduction='mean')  # Compute loss for training\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf_layer.decode(tag_space)\n",
    "        #return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code did not run with error:\n",
    "User\n",
    "$ /bin/python3 /home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\n",
    "Found modelfile data/chunker.pt. Starting decoding.\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 337, in <module>\n",
    "    decoder_output = chunker.decode(opts.inputfile)\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 281, in decode\n",
    "    model.load_state_dict(saved_model['model_state_dict'])\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2153, in load_state_dict\n",
    "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "RuntimeError: Error(s) in loading state_dict for TransformerModel:\n",
    "        Missing key(s) in state_dict: \"crf_layer.start_transitions\", \"crf_layer.end_transitions\", \"crf_layer.transitions\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we removed the pretrained model and ran it again but it gave following error:\n",
    "trying to retrain the model:\n",
    " /bin/python3 /home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\n",
    "Could not find modelfile data/chunker.pt or -f used. Starting training.\n",
    "  0%|                   | 0/8936 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
    "/home/mihir/.local/lib/python3.10/site-packages/torchcrf/__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:519.)\n",
    "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "  0%|          | 15/8936 [00:00<06:39, 22.35it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 332, in <module>\n",
    "    chunker.train()\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 228, in train\n",
    "    loss = loss_function(tag_scores.view(-1, len(self.tag_to_ix)), targets.view(-1))\n",
    "AttributeError: 'list' object has no attribute 'view'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a model with a CRF layer, you typically pass the logits (the output of the classification head before applying softmax) and the true labels to the CRF layer to calculate the negative log-likelihood loss. This means you don't need to calculate the loss separately using loss_function(tag_scores.view(-1, len(self.tag_to_ix)), targets.view(-1)). Instead, you use the CRF layer's own method for computing the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "        self.load_training_data(self.trainfile)\n",
    "        self.model = TransformerModel(self.basemodel, len(self.tag_to_ix), lr=self.lr).to(device)\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            # Optionally freeze the encoder during the first few epochs\n",
    "            if epoch < 2:  # Example: freeze for the first 2 epochs\n",
    "                for param in self.model.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in self.model.encoder.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            train_iterator = tqdm.tqdm(self.training_data)\n",
    "            for tokenized_sentence, tags in train_iterator:\n",
    "                self.model.zero_grad()\n",
    "                inputs, targets = self.prepare_sequence(tokenized_sentence, tags)\n",
    "                attention_mask = inputs['attention_mask'].to(device)\n",
    "                inputs = inputs['input_ids'].to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # The forward pass now returns the loss directly\n",
    "                loss = self.model(inputs, attention_mask=attention_mask, labels=targets)\n",
    "\n",
    "                loss.backward()\n",
    "                for optimizer in self.model.optimizers.values():\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Display average loss for the epoch\n",
    "            avg_loss = total_loss / len(self.training_data)\n",
    "            train_iterator.set_description(f\"Epoch {epoch} Loss: {avg_loss:.3f}\")\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if epoch == self.epochs - 1:\n",
    "                savefile = self.modelfile + self.modelsuffix  # Last epoch\n",
    "            else:\n",
    "                savefile = f\"{self.modelfile}_epoch_{epoch}{self.modelsuffix}\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': {name: opt.state_dict() for name, opt in self.model.optimizers.items()},\n",
    "                'loss': avg_loss,\n",
    "                'tag_to_ix': self.tag_to_ix,\n",
    "                'ix_to_tag': self.ix_to_tag,\n",
    "            }, savefile)\n",
    "            print(f\"Model saved to {savefile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got the following error:\n",
    "$ /bin/python3 /home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\n",
    "Could not find modelfile data/chunker.pt or -f used. Starting training.\n",
    "  0%|                   | 0/8936 [00:00<?, ?it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 301, in <module>\n",
    "    chunker.train()\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 195, in train\n",
    "    loss = self.model(inputs, attention_mask=attention_mask, labels=targets)\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "TypeError: TransformerModel.forward() got an unexpected keyword argument 'attention_mask'\n",
    "\n",
    " which measn there was an issue in forward function and we try to fix it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change\n",
    "def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Encode the inputs\n",
    "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        # Pass the sequence output through the classification head\n",
    "        logits = self.classification_head(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            # If labels are provided, calculate and return the loss using the CRF layer\n",
    "            loss = -self.crf_layer(logits, labels, mask=attention_mask.byte(), reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            # Otherwise, return the decoded sequences from the CRF layer\n",
    "            return self.crf_layer.decode(logits, mask=attention_mask.byte())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got the following error:\n",
    "error:$ /bin/python3 /home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\n",
    "Could not find modelfile data/chunker.pt or -f used. Starting training.\n",
    "  0%|                   | 0/8936 [00:00<?, ?it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 305, in <module>\n",
    "    chunker.train()\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 199, in train\n",
    "    loss = self.model(inputs, attention_mask=attention_mask, labels=targets)\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "  File \"/home/mihir/nlpclass-1241-g-CtrlAltDefeat/hw2/answer/default.py\", line 78, in forward\n",
    "    loss = -self.crf_layer(logits, labels, mask=attention_mask.byte(), reduction='mean')\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torchcrf/__init__.py\", line 90, in forward\n",
    "    self._validate(emissions, tags=tags, mask=mask)\n",
    "  File \"/home/mihir/.local/lib/python3.10/site-packages/torchcrf/__init__.py\", line 155, in _validate\n",
    "    raise ValueError(\n",
    "ValueError: the first two dimensions of emissions and tags must match, got (1, 41) and (41,)\n",
    "\n",
    "We realised that the change made previouly was not managed in train so we did the following change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    self.load_training_data(self.trainfile)\n",
    "    self.model = TransformerModel(self.basemodel, len(self.tag_to_ix), lr=self.lr).to(device)\n",
    "    self.model.train()\n",
    "\n",
    "    for epoch in range(self.epochs):\n",
    "        \n",
    "        if epoch < 2:  \n",
    "            for param in self.model.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in self.model.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        total_loss = 0\n",
    "        train_iterator = tqdm.tqdm(self.training_data, total=len(self.training_data), desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for tokenized_sentence, tags in train_iterator:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            # Prepare inputs and targets\n",
    "            inputs, targets = self.prepare_sequence(tokenized_sentence, tags)\n",
    "            # Ensure inputs, targets, and attention_mask are on the correct device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Ensure targets tensor has a batch dimension\n",
    "            targets = targets.unsqueeze(0) if targets.dim() == 1 else targets\n",
    "\n",
    "            # Forward pass and calculate loss\n",
    "            loss = self.model(input_ids, attention_mask=attention_mask, labels=targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            for optimizer in self.model.optimizers.values():\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.training_data)\n",
    "        print(f\"Average loss for epoch {epoch}: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save model at the end of each epoch\n",
    "        save_path = f\"{self.modelfile}_epoch_{epoch}{self.modelsuffix}\" if epoch < self.epochs - 1 else f\"{self.modelfile}{self.modelsuffix}\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': {name: opt.state_dict() for name, opt in self.model.optimizers.items()},\n",
    "            'loss': avg_loss,\n",
    "            'tag_to_ix': self.tag_to_ix,\n",
    "            'ix_to_tag': self.ix_to_tag,\n",
    "        }, save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to run we decieded to leave CRF as it was taking too much time to train and then we got dev score :90.5883"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The suggested FFN in the classification head without changing anything else in the code:\n",
    "\n",
    "    we need to define a new module, FFN, which includes the dropout and additional linear layers.\n",
    "    Replace the current single linear layer in self.classification_head with this new module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            basemodel,\n",
    "            tagset_size,\n",
    "            lr=5e-5\n",
    "        ):\n",
    "        torch.manual_seed(1)\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.basemodel = basemodel\n",
    "        # the encoder will be a BERT-like model that receives an input text in subwords and maps each subword into\n",
    "        # contextual representations\n",
    "        self.encoder = None\n",
    "        # the hidden dimension of the BERT-like model will be automatically set in the init function!\n",
    "        self.encoder_hidden_dim = 0\n",
    "        # The linear layer that maps the subword contextual representation space to tag space\n",
    "        self.classification_head = None\n",
    "        # The CRF layer on top of the classification head to make sure the model learns to move from/to relevant tags\n",
    "        # self.crf_layer = None\n",
    "        # optimizers will be initialized in the init_model_from_scratch function\n",
    "        self.optimizers = None\n",
    "        self.init_model_from_scratch(basemodel, tagset_size, lr)\n",
    "\n",
    "    def init_model_from_scratch(self, basemodel, tagset_size, lr):\n",
    "        self.encoder = AutoModel.from_pretrained(basemodel)\n",
    "        self.encoder_hidden_dim = self.encoder.config.hidden_size\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(self.encoder_hidden_dim, 3072),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(3072, 768),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(768, tagset_size)\n",
    "        )\n",
    "        # TODO initialize self.crf_layer in here as well.\n",
    "        # TODO modify the optimizers in a way that each model part is optimized with a proper learning rate!\n",
    "        self.optimizers = [\n",
    "            optim.Adam(\n",
    "                list(self.encoder.parameters()) + list(self.classification_head.parameters()),\n",
    "                lr=lr\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def forward(self, sentence_input):\n",
    "        encoded = self.encoder(sentence_input).last_hidden_state\n",
    "        tag_space = self.classification_head(encoded)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
    "        # TODO modify the tag_scores to use the parameters of the crf_layer\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev score:88.0291. Thenthe load_training_data function reads training data from a file, augments each sentence with misspelled words to improve model robustness, and then maps each unique tag found in the data to a unique index. This processed data is then used to train a machine learning model for a natural language processing task. The function also logs the mappings of tags to indices for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(self, trainfile):\n",
    "        augmented_training_data=[]\n",
    "        if trainfile[-3:] == '.gz':\n",
    "            with gzip.open(trainfile, 'rt') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "        else:\n",
    "            with open(trainfile, 'r') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "        for sentence,tags in self.training_data:\n",
    "            augmented_sentence=[self.augment_with_misspellings(word) for word in sentence]\n",
    "            augmented_training_data.append((augmented_sentence,tags))\n",
    "\n",
    "        for sent, tags in self.training_data:\n",
    "            for tag in tags:\n",
    "                if tag not in self.tag_to_ix:\n",
    "                    self.tag_to_ix[tag] = len(self.tag_to_ix)\n",
    "                    self.ix_to_tag.append(tag)\n",
    "\n",
    "        logging.info(\"tag_to_ix:\", self.tag_to_ix)\n",
    "        logging.info(\"ix_to_tag:\", self.ix_to_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev score:88.0291"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since our score was not going up we decided to go back to our previous submition with score dev score :90.5883"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
