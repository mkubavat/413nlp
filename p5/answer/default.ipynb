{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prefixtune: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mihir/hw4/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:14, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0||  ____________________   It's been a long time since I first started writing this blog, so I decided to write a series of posts about the\n",
      "1||  ____________ A few days ago, I received an email from a friend of mine asking me to share my thoughts and thoughts with you. I am\n",
      "2||  _______________________________________________________________________________    I‪This is the first time in a long time that I‪I‪I‪ve\n",
      "3||  中文本語 (本語版)   The latest version of the Linux kernel is now available for Windows Phone\n",
      "4||  ___________________    I’ve been looking for a way to make it easy to use your internet connection when you’re in\n",
      "5||  ____________________________________________________________    The U.S. Department of Homeland Security (DHS) announced today that the Federal Emergency Management Agency (F\n",
      "6||  _______________________________________________________________________________    Welcome to the World of Warcraft World of Warcraft Warcraft World of Warcraft (WOW)      \n",
      "7||  __________________________________________________________________________   It is time to take a quick look at some of the most popular websites and services that we have been using for the\n",
      "8||    ㅠ ㅠ  ㅠ 혁 �\n",
      "9||  中文本中文文中文中文日本語 中文版   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "basemodel = 'distilgpt2'\n",
    "table_to_text = TableToText(\"peft\", basemodel=basemodel)\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel)\n",
    "model.to(device)\n",
    "decoder_output = table_to_text.decode(model, '../data/input/small.txt')\n",
    "print(\"\\n\".join(decoder_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "bleu = sacrebleu.metrics.BLEU(effective_order=True)\n",
    "\n",
    "def compute_bleu(references, output_data):\n",
    "    bleu_score = 0.0\n",
    "    if len(references) == len(output_data):\n",
    "        score = 0.0\n",
    "        total = 0.0\n",
    "        for line in output_data:\n",
    "            r = references[line[0]]\n",
    "            h = line[1]\n",
    "            score += bleu.sentence_score(h, r).score\n",
    "            total += 1.\n",
    "        bleu_score = score / total\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score: 1.0825689376012702\n"
     ]
    }
   ],
   "source": [
    "output = \"\\n\".join(decoder_output)\n",
    "\n",
    "references = {}\n",
    "ref_data = []\n",
    "with open( '../data/reference/small.out', 'r') as ref:\n",
    "    ref_data = list(filter(lambda k: k, [str(x) for x in ref.read().splitlines()]))\n",
    "    for line in ref_data:\n",
    "        src_id, _, suggested_reference = line.split('||')\n",
    "        references.setdefault(src_id, [])\n",
    "        references[src_id].append(suggested_reference)\n",
    "\n",
    "output_data = list(filter(lambda k: k, [str(x) for x in output.splitlines()]))\n",
    "output_data = [line.split('||') for line in output_data]\n",
    "output_data = output_data[:len(ref_data)]\n",
    "\n",
    "print(f\"bleu score: {compute_bleu(references, output_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "We used the PeftModel as it was mentioned in the homework website and there were necesery librarlies already installed in our virtual enviroment. We implemented it in the follwoing way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, PrefixTuningConfig, PeftModel,  TaskType, PeftConfig\n",
    "\n",
    "def train(self):\n",
    "        data_loaders = self.get_data(splits=(\"train\", ))\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.basemodel)\n",
    "\n",
    "        # You can print the parameters for debugging or understanding the code\n",
    "        # but make sure you comment it out otherwise it will pollute the output\n",
    "        # that is produced for dev and test\n",
    "        #model.print_trainable_parameters()\n",
    "        \n",
    "        peft_config = PrefixTuningConfig( task_type= TaskType.CAUSAL_LM ,prefix_projection= self.prefixprojection , inference_mode=False, num_virtual_tokens= self.virtualtokens)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        # model.print_trainable_parameters()\n",
    "\n",
    "        # TODO\n",
    "        # if using HF peft module, then add calls to PrefixTuningConfig and get_peft_model\n",
    "        # which will take num_virtual_tokens which is set to self.virtualtokens and\n",
    "        # prefix_projection which is set to self.prefixprojection\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=(len(data_loaders[\"train\"]) * self.epochs),\n",
    "        )\n",
    "        model = model.to(device)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            # TODO rest of the training steps for prefix tuning\n",
    "            for step, batch in enumerate( tqdm( data_loaders['train'] ) ):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if epoch == self.epochs - 1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            model.save_pretrained(savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply prefix tuning to a pre-trained language model, we first load it using AutoModelForCausalLM.from_pretrained. Next, we configure it with PrefixTuningConfig, specifying our tuning settings. This setup allows us to initialize PeftModel, a wrapper that freezes the original model's weights and introduces additional layers for prefix tuning. The model's training involves accumulating losses through cross-entropy for backpropagation. Due to limited GPU memory, we reduced the batch size to 12. After training, we save the enhanced model structure for later use, which can be reloaded with specific methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we implemented prefix fine tuning we realized that that is not enough to produce relavent results like we can see when runiing small tests below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:02,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0||  The Alimentum is located in the riverside area near the city centre near the city centre. It is not family-friendly and serves English food\n",
      "1||  Near the riverside near the riverside, Alimentum is a family-friendly place with a price range of £20-£25-£\n",
      "2||  Alimentum is located in the riverside area with a price range of £20-£20-£25-£25-£30-\n",
      "3||  The Alimentum is located near the riverside near the riverside area near the riverside. It is family friendly and is located near the rivers\n",
      "4||  The Alimentum is located in the riverside near the riverside. It is located near the riverside and has a price range of £20\n",
      "5||  Alimentum is a family friendly place near the riverside near the riverside called Alimentum. It is located near the riverside with a\n",
      "6||  The Alimentum is located in the riverside.  Located near the riverside.  There is a family friendly atmosphere with a price range of\n",
      "7||  Alimentum is located near the riverside near the riverside.  Located near the riverside, Alimentum in the riverside. \n",
      "8||  There is a child friendly coffee shop located near the riverside near the riverside. It has a customer rating of 1 out of 5. It is\n",
      "9||  There is a family friendly coffee shop near the riverside that has a customer rating of 3 out of 5. It is located near the riverside.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from prefixtune import TableToText\n",
    "table_to_text = TableToText(\"peft\", basemodel=basemodel)\n",
    "decoder_output = table_to_text.decode(model, '../data/input/small.txt')\n",
    "print(\"\\n\".join(decoder_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score: 27.746648542824495\n"
     ]
    }
   ],
   "source": [
    "output = \"\\n\".join(decoder_output)\n",
    "\n",
    "references = {}\n",
    "ref_data = []\n",
    "with open( '../data/reference/small.out', 'r') as ref:\n",
    "    ref_data = list(filter(lambda k: k, [str(x) for x in ref.read().splitlines()]))\n",
    "    for line in ref_data:\n",
    "        src_id, _, suggested_reference = line.split('||')\n",
    "        references.setdefault(src_id, [])\n",
    "        references[src_id].append(suggested_reference)\n",
    "\n",
    "output_data = list(filter(lambda k: k, [str(x) for x in output.splitlines()]))\n",
    "output_data = [line.split('||') for line in output_data]\n",
    "output_data = output_data[:len(ref_data)]\n",
    "\n",
    "print(f\"bleu score: {compute_bleu(references, output_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing prefix tuning improved our BLEU score to 13.97, reducing the irrelavent answers. Adjusting prefix_projection to True slightly raised the BLEU score to 16.79, though the repeated outputs stayed. We tried to fix this by setting no_repeat_ngram_size=2 in the generate method lowered the BLEU score. Reducing max_new_tokens significantly improved the score. Further enhancements involved increasing the beam width to 10 and adjusting the temperature. Experimenting with various virtual_tokens settings showed not much impact. Finally finding a balance between new tokens and n-gram repetition was needed so we tried a setup of 30 new tokens and a 6-gram repetition of 0 gave us the best results, which led ot our final score of 27.74, Below are our final parameters after optimzing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens= 30,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer_pad_token_id,\n",
    "            do_sample=True,\n",
    "            num_beams=10,\n",
    "            top_p=0.9,\n",
    "            temperature= 1.5,\n",
    "            no_repeat_ngram_size= 6,\n",
    "            num_return_sequences=num_sequences\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We ran the code without optimizing but unfortunately we forgot to have the model output in the notebook and since it took 6 hours to train we could not include that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
