{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# spellchk: default program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from default import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Documentation\n",
        "\n",
        "Read `answer/default.py` starting with the `spellchk` function and see how it solves the task of spell correction using a pre-trained language model that can predict a replacement token for a masked token in the input.\n",
        "\n",
        "In your submission, write some beautiful documentation of your program here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\tit will put your mind into non-stop learning.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"4\\tit will put your maind into non-stop learning.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis\n",
        "\n",
        "Do some analysis of the results. What ideas did you try? What worked and what did not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dev score of the default solution was 0.23 \n",
        "\n",
        "The first change we made was to select correction, Select correction is supposed to return the most likely correction for a typo from a given list of predictions, The initial select correction would just select the first item in the list of predictions with no scoring mechanisms or sorting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def select_correction(typo, predict):\n",
        "    # return the most likely prediction for the mask token\n",
        "    for val in predict:\n",
        "        #first change\n",
        "        val[\"combined_len_diff\"] = max(len(set(val[\"token_str\"]) - set(typo)), len(set(typo) - set(val[\"token_str\"]))) + abs(len(val[\"token_str\"]) - len(typo))\n",
        "        val[\"overall_score\"] = - val[\"combined_len_diff\"]\n",
        "    predict = sorted(predict, key = lambda x: x[\"overall_score\"], reverse = True)\n",
        "    return predict[0]['token_str']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\ttherefore I decided to remove the symbols The objects were displaying including ancient artifacts and modren art. whereas the solution for me was the traditional cosmic symbolism where I tried a lot abuot ##uration and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our method we modified select correction to loop over the list of predictions and for each prediction it would calculate the combined length difference between the predicted token and the typo and the difference in the set of letters used. This combined score is then made negative and is applied to the overall score to create a descending order of scores and higher scores indicate better matches. After calculating all the scores we sort to list to keep the higher score at the top and then we choose that value as the prediction\n",
        "\n",
        "Our thought process behind making this change is that select correction just choosing the first item in the list is obviously not going to work so we made changes to it first, Upon thinking of a way to implement this we realized that often times when there is a typo the word is ususally still going to be very similar to the orignal like for example farther and father the letter set is the same so it is good to compare the letter sets between the prediction vales and the typo made, another example would be wera which the model could see as either wear or pear but if we compare the letter sets then wear would win out, The next issue we saw was what if a word has similar letter sets but the lenght of the words was vastly different then our approach of only measuring the letter sets would fail, for example if the typo is biilling then model could come up with bling and billing and both these words would have the same letter set so then we would diffrentiate with the lenght of the letters in which case billing would win out as the correct solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After this change our dev score went to 0.45 as you see from output above it does change the words but not into the correct correction so clearly there is more work to be done\n",
        "\n",
        "we set about trying to find the next area of improvenment which an obvious one was to increase the numbers of predictions (top_k value), we increased it from 20 to 28 This value was obtained by trial and error and just testing different values for top_k. This is the optimal value in the tradeoff between precision and running time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\ttherefore I decided to remove the symbols The objects were displaying including ancient artifacts and modren art. whereas the solution for me was the traditional cosmic symbolism where I tried a lot abuot ##uration and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Changing the top_k to 28 got out dev score up to 0.46\n",
        "\n",
        "The next change we noticed was when we take a look at how the spellchk function works, it corrects the typos in the original sentence by replacing them with the predicted corrections, then it yields the results for each line in the input file. The next change that we tried was"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def spellchk(fh):\n",
        "     for (locations, sent) in get_typo_locations(fh):\n",
        "        spellchk_sent = sent\n",
        "        for i in locations:\n",
        "            # predict top_k replacements only for the typo word at index i\n",
        "            predict = fill_mask(\n",
        "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
        "                top_k=28\n",
        "            )\n",
        "            if i < len(sent):\n",
        "                predict+=fill_mask(\n",
        "                    \" \".join([ sent[:i+1][j] if j != i else mask for j in range(len(sent[:i+1])) ]), \n",
        "                    top_k=28\n",
        "                )\n",
        "            logging.info(predict)\n",
        "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
        "        yield(locations, spellchk_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we extended the prediction context for each typo by incorporating tokens from both the left and right sides of the typo location. Specifically, when predicting replacements for the masked token at the typo index, we added an extra step to include context from the left side as well. This was achieved by constructing a new sentence with the masked token at the typo location and incorporating tokens from the left side. The impact of this change is a more comprehensive set of predictions, considering a broader context around the typo. By expanding the context, the language model has access to additional information, potentially leading to more accurate and contextually appropriate corrections. This modification improved the dev score to 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spellchk(fh):\n",
        "     for (locations, sent) in get_typo_locations(fh):\n",
        "        spellchk_sent = sent\n",
        "        for i in locations:\n",
        "            # predict top_k replacements only for the typo word at index i\n",
        "            predict = fill_mask(\n",
        "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
        "                top_k=35\n",
        "            )\n",
        "            if i < len(sent):\n",
        "                predict+=fill_mask(\n",
        "                    \" \".join([ sent[:i+1][j] if j != i else mask for j in range(len(sent[:i+1])) ]), \n",
        "                    top_k=35\n",
        "                )\n",
        "            logging.info(predict)\n",
        "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
        "        yield(locations, spellchk_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\tyesterday I decided to visit the museum The exhibits were interesting showing things renaissance and modren art. before the thing for me was the picture in question where I learned a lot abuot anatomy and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the above change got us to 0.52. While this adjustment improved our ability to select more suitable replacements for masked words, analysis of test sentence outputs revealed that there were still errors happening. In some instances, the replaced words did not align with our intended choices.\n",
        "\n",
        "To address this issue, we further improved our code. Now, when replacing masked words, the code ensures that the length of the substituted word is more closely aligned with the predicted word. This modification aims to mitigate instances where the length of the replaced word deviates from our intended target, thereby enhancing the overall accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spellchk(fh):\n",
        "     for (locations, sent) in get_typo_locations(fh):\n",
        "        corrected_sent = sent[:]\n",
        "        for i in locations:\n",
        "            masked_sentence = \" \".join([sent[j] if j != i else mask for j in range(len(sent))])\n",
        "            predictions = fill_mask(masked_sentence, top_k=28)\n",
        "            corrected_sent[i] = select_correction(sent[i], predictions)\n",
        "        yield (locations, corrected_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\there I decided to find the dust The effects were plainly suggesting the ##ics and modren art. for the sight for me was the perfect ##ne ##n where I learned a lot abuot ##ure and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dev score remains at 0.52, and the word replacements did not align with our intentions. Additionally, some replaced words contain \"##\". In response to these challenges, we did some research and identified the usefullness of the levenshtein score. To use this, we faced a choice between creating a custom helper function or importing a library. Given the uncertainty about adding another library, we opted to implement a dedicated helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def levenshtein_distance(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1)\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    \n",
        "    return previous_row[-1]\n",
        "\n",
        "def select_correction(typo, predict):\n",
        "    # return the most likely prediction for the mask token\n",
        "    # Apply Levenshtein distance to find the closest prediction\n",
        "    closest_prediction = min(predict, key=lambda x: levenshtein_distance(typo, x[\"token_str\"]))\n",
        "    return closest_prediction[\"token_str\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\there I decided to find the dust The effects were plainly suggesting the ##ics and modren art. for the sight for me was the perfect ##ne ##n where I learned a lot abuot ##ure and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The updated score is now at 0.56, reflecting an improvement, but theres still the same problem that the words are not replacing properly, so we decided to improve our spellchk function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spellchk(fh):\n",
        "     for (locations, sent) in get_typo_locations(fh):\n",
        "        spellchk_sent = sent.copy()  # Create a copy of the sentence tokens\n",
        "        for i in locations:\n",
        "            # Formulate the sentence with a masked token for the current typo\n",
        "            masked_sentence = \" \".join(sent[j] if j != i else mask for j in range(len(sent)))\n",
        "            # Generate predictions for the masked token\n",
        "            predict = fill_mask(masked_sentence, top_k=50)  # Increased top_k for a broader selection\n",
        "\n",
        "            # Select the best correction based on the enhanced selection logic\n",
        "            correction = select_correction(sent[i], predict)\n",
        "            spellchk_sent[i] = correction\n",
        "        \n",
        "        yield (locations, spellchk_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\tyesterday I decided to view the dust The effects were painted showing the ##ics and modren art. for the sight for me was the perfect ##ence ##n where I learned a lot abuot ##ron and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Increasing the top_k value increased our dev score to 0.57, so we've made progress, but the primary concern persists: strings containing \"##.\" To address this, we implemented the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_correction(typo, predict):\n",
        "    # Assuming 'predict' includes a confidence score along with the token string\n",
        "    filtered_predict = [p for p in predict if \"##\" not in p[\"token_str\"]]\n",
        "    \n",
        "    if not filtered_predict:\n",
        "        filtered_predict = predict\n",
        "    \n",
        "    # Combine Levenshtein distance and model confidence to select the best correction\n",
        "    def combined_score(prediction):\n",
        "        levenshtein_score = levenshtein_distance(typo, prediction[\"token_str\"])\n",
        "        confidence_score = prediction.get(\"score\", 0)  # Use model confidence if available\n",
        "        return levenshtein_score - confidence_score  # Adjust weights as necessary\n",
        "    \n",
        "    closest_prediction = min(filtered_predict, key=combined_score)\n",
        "    return closest_prediction[\"token_str\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\tyesterday I decided to view the dust The effects were painted showing the shapes and modren art. for the sight for me was the perfect lens , where I learned a lot abuot , and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dev score of 0.57. We did sucessfully solve the issue with the \"##\" strings however the score did not go up by as much as we expected so while fine tuning we further increased the top_k score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spellchk(fh):\n",
        "     for (locations, sent) in get_typo_locations(fh):\n",
        "        spellchk_sent = sent.copy()  # Create a copy of the sentence tokens\n",
        "        for i in locations:\n",
        "            # Formulate the sentence with a masked token for the current typo\n",
        "            masked_sentence = \" \".join(sent[j] if j != i else mask for j in range(len(sent)))\n",
        "            # Generate predictions for the masked token\n",
        "            predict = fill_mask(masked_sentence, top_k=180)  # Increased top_k for a broader selection\n",
        "\n",
        "            # Select the best correction based on the enhanced selection logic\n",
        "            correction = select_correction(sent[i], predict)\n",
        "            spellchk_sent[i] = correction\n",
        "        \n",
        "        yield (locations, spellchk_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\tyesterday I decided to visit the masses The bits were painting showing ancient crystals and modren art. however the highlight for me was the treatise lens thing where I learned a lot abuot anatomy and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Increasing the top_k score further got us to a 0.65 and we tried a few other methods after this but none were too rewarding and we unfortunately  ran out of time to further improve the code here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_correction(typo, predict):\n",
        "    def combined_score(prediction):\n",
        "        weight_levenshtein = 1.0\n",
        "        weight_confidence = 1.0 \n",
        "        levenshtein_score = levenshtein_distance(typo, prediction[\"token_str\"])\n",
        "        confidence_score = prediction.get(\"score\", 0) \n",
        "        normalized_levenshtein = levenshtein_score / max(len(typo), len(prediction[\"token_str\"]))\n",
        "        return (weight_levenshtein * normalized_levenshtein) - (weight_confidence * confidence_score)\n",
        "    \n",
        "    filtered_predict = [p for p in predict if \"##\" not in p[\"token_str\"]]\n",
        "\n",
        "    if not filtered_predict:\n",
        "        filtered_predict = predict\n",
        "        \n",
        "    closest_prediction = min(filtered_predict, key=combined_score)\n",
        "    return closest_prediction[\"token_str\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\tyesterday I decided to visit the masses The bits were painting showing ancient crystals and modren art. however the sight for me was the treatise lens thing where I learned a lot abuot anatomy and biollogy.\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "with StringIO(\"0,2,4,6,8,10,11,12,13,17,19,24,25,26,27,29,30,31,33\\tYestrday, I decded to vissit the musuem. The exhbits were fasinating, showcassing anshent artifacs and modren art. Howevr, the highligt for me was the interractive siense secction, wher I lernd a lot abuot astronomey and biollogy.\") as f:\n",
        "    for (locations, spellchk_sent) in spellchk(f):\n",
        "        print(\"{locs}\\t{sent}\".format(\n",
        "            locs=\",\".join([str(i) for i in locations]),\n",
        "            sent=\" \".join(spellchk_sent)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above you can see that we tried to change the weights and further fine-tune but out results reamined the same or lower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Group work\n",
        "We worked together in the same room for a lot of the project however below is the split of the work.\n",
        "* mkubavat mostly did the testing and coding for default.py and helped with comments in the juptyer notebook.\n",
        "* yserai helped with fine-tuning and idea generation,and mostly made the report and juptyer notebook.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
